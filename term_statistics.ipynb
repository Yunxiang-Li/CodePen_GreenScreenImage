{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "term-statistics.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yunxiang-Li/CodePen_GreenScreenImage/blob/master/term_statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yYCQrU0J5C5"
      },
      "source": [
        "# Working with Terms and Documents\n",
        "\n",
        "This first homework assignment starts off with term statistics computations and graphing. In the final section (for CS6200 students), you collect new documents to experiment with.\n",
        "\n",
        "Read through this Jupyter notebook and fill in the parts marked with `TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA4bCPS1MmAx"
      },
      "source": [
        "## Sample Data\n",
        "\n",
        "Start by looking at some sample data. We donwload the counts of terms in documents for the first one million tokens of a newswire collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-IMINS4IFUg"
      },
      "source": [
        "!wget -O ap201001.json.gz https://github.com/dasmiq/cs6200-hw1/blob/main/ap201001.json.gz?raw=true\n",
        "!gunzip ap201001.json.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SEFjGZvM4lY"
      },
      "source": [
        "We convert this file with one JSON record on each line to a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CrLD5SOIMz1"
      },
      "source": [
        "import json\n",
        "rawfile = open('ap201001.json')\n",
        "terms = [json.loads(line) for line in rawfile]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg4P9XJJM_lZ"
      },
      "source": [
        "Here are the first ten records, showing the count of each term for each document and field. In this dataset, field only takes the values `body` or `title`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8GdOuyzI0wm",
        "outputId": "21700c53-ecdd-4077-e535-7e5e296206ac"
      },
      "source": [
        "terms[1:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'about'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'abuse'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'academy'},\n",
              " {'count': 2,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'accused'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'actress'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'ad'},\n",
              " {'count': 1, 'field': 'body', 'id': 'APW_ENG_20100101.0001', 'term': 'after'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'agenda'},\n",
              " {'count': 1,\n",
              "  'field': 'body',\n",
              "  'id': 'APW_ENG_20100101.0001',\n",
              "  'term': 'agreed'}]"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-CjFLXH3BJg"
      },
      "source": [
        "Each record has four fields:\n",
        "* `id`, with the identifier for the document;\n",
        "* `field`, with the region of the document containing a given term;\n",
        "* `term`, with the lower-cased term; and\n",
        "* `count`, with the number of times each term occurred in that field and document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H5yBvEVNUPr"
      },
      "source": [
        "## Computing Term Statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDt23kKv0Uy"
      },
      "source": [
        "If we look at the most frequent terms for a given document, we mostly see common function words, such as `the`, `and`, and `of`. Start exploring the dataset by computing some of these basic term statistics. You can make your life easier using data frame libraries such as `pandas`, core python libraries such as `collections`, or just simple list comprehensions.\n",
        "\n",
        "Feel free to define helper functions in your code before computing the statistics we're looking for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Zy5qR562nZ5"
      },
      "source": [
        "# TODO: Print the 6 terms from document APW_ENG_20100101.0001 with the highest count."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7BvW8NR2ynC"
      },
      "source": [
        "# TODO: Print the 10 terms from all fields of document APW_ENG_20100102.0077 with the highest count."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7OwCo0w5R1q"
      },
      "source": [
        "# TODO: Print the 10 terms with the highest total count in the corpus."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnNEUACW23Dd"
      },
      "source": [
        "Raw counts may not be the most informative statistic. One common improvement is to use *inverse document frequency*, the inverse of the proportion of documents that contain a given term."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiUA502P2QkH"
      },
      "source": [
        "# TODO: Compute the number of distinct documents in the collection.\n",
        "N = 0\n",
        "\n",
        "# TODO: Compute the number of distinct documents each term appears in\n",
        "# and store in a dictionary.\n",
        "df = dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XMPAKYNCq6Y"
      },
      "source": [
        "# TODO: Print the relative document frequency of 'the',\n",
        "# i.e., the number of documents that contain 'the' divided by N."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohFmwtc7Chy3"
      },
      "source": [
        "Empricially, we usually see better retrieval results if we rescale term frequency (within documents) and inverse document frequency (across documents) with the log function. Let the `tfidf` of term _t_ in document _d_ be:\n",
        "```\n",
        "tfidf(t, d) = log(count(t, d) + 1) * log(N / df(t))\n",
        "```\n",
        "\n",
        "Later in the course, we will show a probabilistic derivation of this quantity based on smoothing language models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fmyj4v_uHdyo"
      },
      "source": [
        "# TODO: Compute the tf-idf value for each term in each document.\n",
        "# Take the raw term data and add a tfidf field to each record.\n",
        "tfidf_terms = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlXQmMO9HxH0"
      },
      "source": [
        "# TODO: Print the 20 term-document pairs with the highest tf-idf values."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61xitl1IApl"
      },
      "source": [
        "## Plotting Term Distributions\n",
        "\n",
        "Besides frequencies and tf-idf values within documents, it is often helpful to look at the distrubitions of word frequencies in the whole collection. In class, we talk about the Zipf distribution of word rank versus frequency and Heaps' Law relating the number of distinct words to the number of tokens.\n",
        "\n",
        "We might examine these distributions to see, for instance, if an unexpectedly large number of very rare terms occurs, which might indicate noise added to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsM5k1_5Jj7Y"
      },
      "source": [
        "# TODO: Compute a list of the distinct words in this collection and sort it in descending order of frequency.\n",
        "# Thus frequency[0] should contain the word \"the\" and the count 62216.\n",
        "frequency = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdtc14EULkxS"
      },
      "source": [
        "# TODO: Plot a graph of the log of the rank (starting at 1) on the x-axis,\n",
        "# against the log of the frequency on the y-axis. You may use the matplotlib\n",
        "# or other library."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the number of tokens in the corpus.\n",
        "# Remember to count each occurrence of each word. For instance, the 62,216\n",
        "# instances of \"the\" will all count here.\n",
        "ntokens = 0"
      ],
      "metadata": {
        "id": "-WdHjFCSC7WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the proportion of tokens made up by the top 10 most\n",
        "# frequent words."
      ],
      "metadata": {
        "id": "V_7wOcqKAz9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Compute the proportion of tokens made up by the words that occur\n",
        "# exactly once in this collection."
      ],
      "metadata": {
        "id": "uF-1VxcZBXMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdiUXaXZMFqT"
      },
      "source": [
        "## Acquiring New Documents (for CS6200)\n",
        "\n",
        "For this assignment so far, you've worked with data that's already been extracted, tokenized, and counted. In this final section, you'll briefly explore acquiring new data.\n",
        "\n",
        "Find a collection of documents that you're interested in. For the statistics to be meaningful, this collection should have at least 1,000 words.\n",
        "\n",
        "The format could be anything you can extract text from: HTML, PDF, MS PowerPoint, chat logs, etc.\n",
        "\n",
        "The collection should be in a natural language, not mostly code or numerical data. It could be in English or in any other language.\n",
        "\n",
        "The final project for this course will involve designing an information retrieval task on some dataset. You could use this exercise to think about what kind of data you might be interested in, although that is not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGPVnXv2O6AN"
      },
      "source": [
        "**TODO**: Write code to download and extract the text from the collection. Describe choices you make about what contents to keep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeDDz1SaPLik"
      },
      "source": [
        "# TODO: Data acquisition code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-oRTixPKbw"
      },
      "source": [
        "**TODO**: Write code to tokenize the text and count the resulting terms in each document. Describe your tokenization approach here.\n",
        "\n",
        "Each term may also be associated with a field, such as `body` and `title` in the newswire collection above. Describe the different fields in your data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ydsh0h74Pnlh"
      },
      "source": [
        "# TODO: Tokenization code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-boFCotQ4Ur"
      },
      "source": [
        "**TODO**: Plot a graph of the log rank against log frequency for your collection, as you did for the sample collection above. What do you observe about the differences between the distributions in these two collections?"
      ]
    }
  ]
}